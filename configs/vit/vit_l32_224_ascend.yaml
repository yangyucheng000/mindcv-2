# Copyright 2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

# system config
mode: 0
distribute: True
num_parallel_workers: 8
val_while_train: True
val_interval: 1

# dataset config
dataset: "imagenet"
data_dir: "/data/imagenet"
shuffle: True
dataset_download: False
batch_size: 128 # diff from paper
drop_remainder: True

# Augmentation config
image_resize: 224
scale: [0.08, 1.0]
ratio: [0.75, 1.333]
hflip: 0.5
interpolation: "bicubic"
re_prob: 0.1 # 1118 part2, compare to 1118 part1
mixup: 0.2
cutmix: 1.0
cutmix_prob: 1.0
crop_pct: 0.875
color_jitter: [0.4, 0.4, 0.4]
auto_augment: "randaug-m7-mstd0.5" # 1118 part2

# model config
model: "vit_l_32_224"
drop_rate: 0.1
drop_path_rate: 0.1
num_classes: 1000
pretrained: False
ckpt_path: ""
keep_checkpoint_max: 10
ckpt_save_policy: "top_k"
ckpt_save_dir: "./vit_l32_224"
epoch_size: 300
dataset_sink_mode: True
amp_level: "O2"

# loss config
loss: "CE"
loss_scale: 1024.0
label_smoothing: 0.1 # Exp 1119, compared to 1118 part2

# lr scheduler config
scheduler: "warmup_cosine_decay"
lr: 0.0015 #
min_lr: 1e-6
warmup_epochs: 32
decay_epochs: 268
lr_epoch_stair: False

# optimizer config
opt: "adamw"
weight_decay: 0.025
filter_bias_and_bn: True
use_nesterov: False
